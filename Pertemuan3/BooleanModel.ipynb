{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93613f61-b897-46de-9afa-428e9728438a",
   "metadata": {},
   "source": [
    "<img src=\"data/1_1FMoK_HWvk1IBaMelUXibw.webp\" style=\"height:300px\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd86a33-0ffd-48d2-9056-c7574bf13168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')   # Unduh daftar kata umum (stopwords)\n",
    "nltk.download('punkt')       # Unduh tokenizer untuk pemisahan kata/kalimat\n",
    "nltk.download('wordnet')     # Unduh data untuk lemmatization (bentuk dasar kata)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer  # Untuk normalisasi kata\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize   # Untuk memecah teks\n",
    "\n",
    "import glob, re, os, numpy as np, sys                    # Import modul pendukung\n",
    "\n",
    "Stopwords = set(stopwords.words('english'))              # Simpan daftar stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e13e23-7240-4a95-81c4-864f003c0854",
   "metadata": {},
   "source": [
    "Implementing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0b41d0-8d2c-4235-b9ef-5fd8aeb1da27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_10800\\3526100467.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  regex = re.compile('[^a-zA-Z0-9\\s]')   # Hapus karakter selain huruf & angka\n"
     ]
    }
   ],
   "source": [
    "def finding_all_unique_words_and_freq(words):\n",
    "    words_unique = []                      # Simpan kata unik\n",
    "    word_freq = {}                         # Simpan frekuensi tiap kata\n",
    "    for word in words:\n",
    "        if word not in words_unique:       # Cek kata belum ada\n",
    "            words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "        word_freq[word] = words.count(word) # Hitung kemunculan kata\n",
    "    return word_freq\n",
    "\n",
    "def finding_freq_of_word_in_doc(word, words):\n",
    "    freq = words.count(word)               # Hitung frekuensi kata di dokumen\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')   # Hapus karakter selain huruf & angka\n",
    "    text_returned = re.sub(regex, '', text)\n",
    "    return text_returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3936f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\1_1FMoK_HWvk1IBaMelUXibw.webp\n",
      "data\\badminton.txt\n",
      "data\\barack obama.txt\n",
      "data\\baseball.txt\n",
      "data\\lee quan yew.txt\n",
      "data\\narendra modi.txt\n",
      "data\\queen elizabeth.txt\n",
      "data\\shinzo abe.txt\n",
      "data\\table tennis.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_10800\\3212717052.py:13: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  text = re.sub(re.compile('\\d'), '', text)# Hapus angka\n"
     ]
    }
   ],
   "source": [
    "all_words = []                # Menyimpan semua kata dari file  \n",
    "dict_global = {}              # Menyimpan kata unik dan frekuensinya  \n",
    "file_folder = 'data/*'        # Lokasi file teks  \n",
    "idx = 1\n",
    "files_with_index = {}         # Indeks dan nama file  \n",
    "\n",
    "for file in glob.glob(file_folder):          # Loop tiap file  \n",
    "    print(file)\n",
    "    with open(file, \"r\", encoding=\"latin-1\") as f:\n",
    "        text = f.read()                      # Baca isi file  \n",
    "\n",
    "    text = remove_special_characters(text)   # Hapus karakter khusus  \n",
    "    text = re.sub(re.compile('\\d'), '', text)# Hapus angka  \n",
    "    sentences = sent_tokenize(text)          # Pisah teks jadi kalimat  \n",
    "    words = word_tokenize(text)              # Pisah kalimat jadi kata  \n",
    "    words = [word for word in words if len(word) > 1]  # Filter kata pendek  \n",
    "    words = [word.strip().lower() for word in words]   # Ubah ke huruf kecil  \n",
    "    words = [word for word in words if word not in Stopwords]  # Hapus stopword  \n",
    "    dict_global.update(finding_all_unique_words_and_freq(words)) # Hitung frekuensi  \n",
    "    files_with_index[idx] = os.path.basename(file)     # Simpan nama file  \n",
    "    idx += 1\n",
    "\n",
    "unique_words_all = set(dict_global.keys())   # Kumpulan kata unik  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f7584e-7a8e-4824-a547-0753f8e31cf4",
   "metadata": {},
   "source": [
    "Finding the set of unique words from all documents of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebae2f-46dd-4e54-ad45-f21382a01cea",
   "metadata": {},
   "source": [
    "Defining the linked list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba2475f-1013-4cbc-b9b7-007766d893a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Node menyimpan ID dokumen dan frekuensi kata\n",
    "class Node:\n",
    "    def __init__(self, docId, freq=None):\n",
    "        self.freq = freq        # Frekuensi kata dalam dokumen\n",
    "        self.doc = docId        # ID dokumen\n",
    "        self.nextval = None     # Pointer ke node berikutnya\n",
    "\n",
    "# SlinkedList digunakan untuk membentuk daftar terhubung (linked list)\n",
    "class SlinkedList:\n",
    "    def __init__(self, head=None):\n",
    "        self.head = head        # Menyimpan node awal dari linked list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8513f-75df-4277-a684-a91c551ea399",
   "metadata": {},
   "source": [
    "Making a linkedlist for each word and storing all the nodes (containing the file name and frequency of the respective word ) in the linkedlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeaea509-d82e-4679-bbfd-93d65b8d8c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah file: 9\n",
      "Jumlah kata unik: 1356\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ambil stopwords bahasa Inggris dari NLTK\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Hapus karakter selain huruf, angka, dan spasi\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Inisialisasi variabel utama\n",
    "file_folder = 'data/*'      # Folder berisi file teks\n",
    "dict_global = {}            # Menyimpan semua kata & frekuensinya\n",
    "files_with_index = {}       # Mapping index file â†’ nama file\n",
    "idx = 1                     # Penanda index file\n",
    "\n",
    "# Loop membaca setiap file di folder\n",
    "for file_path in glob.glob(file_folder):\n",
    "    with open(file_path, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Bersihkan teks\n",
    "    text = remove_special_characters(text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.strip().lower() for w in words if len(w) > 1 and w.lower() not in Stopwords]\n",
    "\n",
    "    # Hitung frekuensi kata\n",
    "    for w in words:\n",
    "        dict_global[w] = dict_global.get(w, 0) + 1\n",
    "\n",
    "    # Simpan nama file dengan index\n",
    "    files_with_index[idx] = os.path.basename(file_path)\n",
    "    idx += 1\n",
    "\n",
    "# Ambil semua kata unik\n",
    "unique_words_all = set(dict_global.keys())\n",
    "\n",
    "# Tampilkan hasil ringkasan\n",
    "print(f\"Jumlah file: {len(files_with_index)}\")      # Banyaknya dokumen\n",
    "print(f\"Jumlah kata unik: {len(unique_words_all)}\") # Banyaknya kata unik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32a56b-b2d7-40b2-8885-143d5a979710",
   "metadata": {},
   "source": [
    "Query processing and output generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ecadb9-7fe8-4321-a736-2d5aa213130a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total kata unik dengan linked list: 1356\n",
      "Contoh kata dan dokumen tempat dia muncul:\n",
      "\n",
      "Kata: modi\n",
      "  -> Dokumen 6 (frekuensi 2)\n",
      "\n",
      "Kata: rizyr\n",
      "  -> Dokumen 1 (frekuensi 1)\n",
      "\n",
      "Kata: sa\n",
      "  -> Dokumen 1 (frekuensi 1)\n",
      "\n",
      "Kata: qf\n",
      "  -> Dokumen 1 (frekuensi 1)\n",
      "\n",
      "Kata: yasuo\n",
      "  -> Dokumen 8 (frekuensi 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary untuk menyimpan kata unik beserta linked list-nya\n",
    "linked_list_data = {}\n",
    "\n",
    "# Inisialisasi linked list kosong untuk setiap kata unik\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = SlinkedList()\n",
    "\n",
    "# Loop ke semua file dalam koleksi dokumen\n",
    "for idx, file in files_with_index.items():\n",
    "    # Buka file berdasarkan nama file yang disimpan sebelumnya\n",
    "    with open(os.path.join('data', file), \"r\", encoding=\"latin-1\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Bersihkan teks dari karakter spesial dan angka\n",
    "        text = remove_special_characters(text)\n",
    "        text = re.sub(r'\\d', '', text)\n",
    "\n",
    "        # Tokenisasi menjadi kata-kata\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Normalisasi kata: huruf kecil, panjang > 1, bukan stopword\n",
    "        words = [w.strip().lower() for w in words if len(w) > 1 and w.lower() not in Stopwords]\n",
    "\n",
    "        # Hitung frekuensi tiap kata di dokumen ini\n",
    "        freq_dict = finding_all_unique_words_and_freq(words)\n",
    "\n",
    "        # Untuk setiap kata, buat node baru dan tambahkan ke linked list\n",
    "        for word, freq in freq_dict.items():\n",
    "            if word in linked_list_data:\n",
    "                # Buat node baru dengan menyimpan ID dokumen & frekuensi kata\n",
    "                new_node = Node(idx, freq)\n",
    "\n",
    "                # Jika linked list kosong, jadikan node ini sebagai head\n",
    "                if linked_list_data[word].head is None:\n",
    "                    linked_list_data[word].head = new_node\n",
    "                else:\n",
    "                    # Jika sudah ada data sebelumnya, tambahkan di akhir linked list\n",
    "                    current = linked_list_data[word].head\n",
    "                    while current.nextval is not None:\n",
    "                        current = current.nextval\n",
    "                    current.nextval = new_node\n",
    "\n",
    "print(f\"Total kata unik dengan linked list: {len(linked_list_data)}\")\n",
    "print(\"Contoh kata dan dokumen tempat dia muncul:\")\n",
    "\n",
    "# Contoh: tampilkan 5 kata pertama untuk memastikan hasil benar\n",
    "for i, (word, sll) in enumerate(linked_list_data.items()):\n",
    "    print(f\"\\nKata: {word}\")\n",
    "    node = sll.head\n",
    "    while node is not None:\n",
    "        print(f\"  -> Dokumen {node.doc} (frekuensi {node.freq})\")\n",
    "        node = node.nextval\n",
    "    if i == 4:  # batasi biar gak kepanjangan\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28590aa-7b5c-407e-8420-ae808e0e4af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
